''' Import Excel with information on the mapping of features and CAS.
    The table is complemented and validated using google and PubChem.
    The resulting pandas dataframe is stored as pickle file.'''

# Script to generate table with default values and alias used by NGC.ner
# and store it as pickle.
# Note: Excel makes errors when saving as CSV
# For example: some symbols (e.g. latin symbols) are saved incorretly as "?"

# %% Import packages

import datetime
import googlesearch
import hgc.named_entity_recognition.utils as named_entity_recognition.utils
import hgc.nersearch as nersearch
import itertools
import numpy as np
import openpyxl
import os
import pandas as pd
import pickle
import pubchempy as pcp
import re
import time

from fuzzywuzzy import fuzz
from fuzzywuzzy import process
from google_trans_new import google_translator
from pathlib import Path

import urllib




# %% Defaults

# set work directory to current module
PATH = Path(__file__).parent
os.chdir(PATH)

# import
FILE_EXCEL = PATH / 'feature_map_before_import_20210305.xlsx'
SKIPROWS = 4

SHEET_ALIAS = 'Alias'
COLS_ALIAS_EN = ['HGC', 'MicrobialParameters', 'OtherEnglishSynonym']
COLS_ALIAS_NL = ['OtherDutchSynonym', 'SIKB', 'NVWA', 'REWAB']
COLS_CAS = ['Other_CASnumber', 'SIKB_CASnumber', 'NVWA_CASnumber', 'REWAB_CASnumber']
COLS_DBASE = ['SIKBcode', 'KIWAnumber']

# import/ export intermediate results
FILE_FEATURE_NER = PATH / 'feature_ner.pkl'
FILE_CAS_NER = PATH / 'cas_ner.pkl'
FILE_NL2ALIAS = PATH / 'nl2alias.pkl'  # dictionary Dutch --> English
FILE_CID2COMPOUND = PATH / 'cid2compound.pkl'

# export
FILE_ALIAS = PATH / 'feature_map.pkl'
FILE_ALIAS2CSV = PATH / 'feature_map.csv'


# cols generated by script
COLS_GENERATED = ['ValidCas', 'ValidCid', 'BestCas', 'BestCid', 'SynonymEnglish', 'SynonymDutch',
                  'Warnings', 'NrOfRowsMerged']

GOOGLE_SEARCH_MAXTIME = 0 * 60.  # max duration of google search in seconds


# %% Functions


def _split_string_in_dfcol_to_list(df=None, cols=[], separator='|'):
    """ Convert all items in selected columns of dataframe to list. Assumes vertical bar "|" as splitter."""
    for col in cols:
        items = []
        for item in df[col]:
            if len(item) > 0:
                item = [x.strip() for x in item.split(separator)]
            else:
                item = []
            items.append(item)
        df[col] = items


def _list_in_df_to_lists(df):
    """ Convert columns of dataframe to a single list. All columns must contain lists."""
    nestlst = [list(df.explode(col)[col]) for col in df.columns]  # 1 list per columns]
    lst = list(itertools.chain.from_iterable(nestlst))
    return lst


# def check_pubchemcompound_lst(lst_alias=[], alias2ident={}, ident2cid={}, cid2compound={},
#                                       alias2google= {}, google2ident= {}, match_method='pubchem'):
#     """
#     Check for a list of alias (or Cas) if dictionaries are completely mapped to compounds.

#     Parameters
#     ----------
#     lst_alias : list of str
#         list of strings to check.
#     dictionaries
#         option 1: cas2ident -> ident2cid -> cid2compound
#         option 2: alias2ident -> ident2cid -> cid2compound
#         option 3: alias2google -> google2ident -> ident2cid -> cid2compound
#     match_method: str
#         option 1: pubchem_cas
#         option 2: pubchem
#         option 3: pubchem_google
#         see function ner.feature_map()

#     Return
#     ------
#     missing_alias = list of str
#         list of mapped alias/ Cas

#     """
#     missing_alias = []
#     for alias in lst_alias:
#         try:
#             if match_method == 'pubchem_cas':
#                 if cas2ident[alias] == '':  # Empty string means that no compound was found in pubchem -> okay.
#                     cids = []
#                 else:
#                     cids = ident2cid[cas2ident[alias]]  # KeyError if key does not exist
#             elif match_method == 'pubchem':
#                 if alias2ident[alias] == '':
#                     cids = []
#                 else:
#                     cids = ident2cid[alias2ident[alias]]
#             elif match_method == 'pubchem_google':
#                 if alias2google[alias] == '':
#                     cids = []
#                 else:
#                     cids = ident2cid[google2ident[alias2google[alias]]]
#             else:
#                 ValueError("function has an invalid argument")
#             for cid in cids:
#                 if type(cid2compound[cid]) != pcp.Compound: #  # TypeError if not a compound
#                     raise TypeError
#         except (TypeError, KeyError):
#             missing_alias.append(alias)

#     return missing_alias

def get_pubchemcid_from_features(synonym_orig=[], match_method='feature_pubchemcid', cid2compound={}):
    """ Find compounds in PubChem for list of features and return as dataframe (Note: cid2compound is adjusted inline)."""
    # initialise
    df = pd.DataFrame([])
    n = len(lst_synonyms)
    i = 0

    # loop trough list of synonyms
    for syn in lst_synonyms:
        # Search PubChem
        compounds, identifier = nersearch.get_pubchemcompound(syn)

        # Update dataframe
        dct = {'synonym_orig3': [syn], 'ident': [''], 'match_method': [match_method]}  # result in case no match found
        if (identifier != ''):
            dct = {**dct,
                   'ident': [identifier],
                   'cids': [[compound.cid for compound in compounds]],
                   'attribs': [{compound.cid: nersearch._get_pubchem_attributes(compound) for compound in compounds}]
                   }
            if save_compound != False:
                save_compound = {**save_compound, **{compound.cid: compound for compound in compounds}}

        df = pd.concat([df, pd.DataFrame(dct)])

        if i % 20 == 0:
            print(match_method, ' ', i, ' of ', n)
        i += 1

    return df


def get_pubchemcid_from_cas(lst_cas=[], match_method='cas_pubchemcid', cid2compound={}):
    """ Find compounds in PubChem for list of CAS and return as dataframe (Note: cid2compound is adjusted inline)."""
    df = get_pubchemcid_from_features(lst_cas=[], match_method=match_method, save_compound=save_compound)
    df.rename({'synonym_orig3': 'cas_orig'})
    return df


def google_pubchemcompound_featues(lst_synonyms=[], match_method='google_pubchemcid', cid2compound={},
                                maxtime=GOOGLE_SEARCH_MAXTIME):
    """ Find compounds in PubChem for list of features using google (and in-line update dictionaries)."""
    df = pd.DataFrame([])
    HTTPError_status = 0
    n = len(lst_alias)  # total number of features
    i = 0
    starttime = float(time.time())
    endtime = starttime + maxtime

    # loop trough list of synonyms
    for syn in lst_synonyms:
        if time.time() > endtime:
            break
        else:
            # search google
            google_result, HTTPError_status = nersearch._google_pubchemcompound(alias, HTTPError_status=HTTPError_status)
            compounds, google_identifier = nersearch.get_pubchemcompound(google_result)  # identifier that matches google search

            # Update dataframe
            dct = {'synonym_orig3': [syn], 'ident': [''], 'match_method': [match_method]}
            if (isinstance(google_result, str)):
                dct[alias] = [google_result]
                if (google_identifier != ''):
                    dct = {**dct,
                           'ident': [google_identifier],
                           'cids': [[compound.cid for compound in compounds]],
                           'attribs': [{compound.cid: nersearch._get_pubchem_attributes(compound) for compound in compounds}]
                           }
                    if save_compound != False:
                        save_compound = {**save_compound, **{compound.cid: compound for compound in compounds}}

        df = pd.concat([df, pd.DataFrame(dct)])

        if i % 2 == 0:
            print(match_method, ' ',  i, 'of', n, '. time left: ' + str(int(endtime - time.time())))
        i += 1

    return df



# def get_pubchemcompound_feature_lst(lst_alias=[], alias2ident={}, ident2cid={}, cid2compound={},
#                                     match_method='feature_pubchem'):
#     """
#     Find compounds in PubChem for list of features (and in-line update dictionaries).

#     Parameters
#     ----------
#     see function: check_pubchemcompound_feature_lst()

#     Return
#     ------
#     Update of dictionaries

#     """
#     n = len(lst_alias)
#     i = 0
#     for alias in lst_alias:
#         # Search PubChem
#         compounds, identifier = nersearch.get_pubchemcompound(alias)

#         # Update dictionaries
#         alias2ident[alias] = identifier
#         if (identifier != ''):
#             ident2cid[identifier] = [compound.cid for compound in compounds]
#             for compound in compounds:
#                 cid2compound[compound.cid] = compound
#         if i % 20 == 0:
#             print(match_method, ' ', i, ' of ', n)
#         i += 1


# def get_pubchemcompound_cas_lst(lst_cas=[], cas2ident={}, ident2cid={}, cid2compound={},
#                                 match_method='cas_pubchem'):
#     """Find compounds in PubChem for list of features (and in-line update dictionaries)."""
#     get_pubchemcompound_feature_lst(lst_alias=lst_cas, alias2ident=cas2ident, ident2cid=ident2cid,
#                                     cid2compound=cid2compound, match_method=match_method)


# def google_pubchemcompound_lst(lst_alias [], alias2google={}, google2googleident={},
#                                 google2ident={}, ident2cid={}, cid2compound={},
#                                 FailedGoogleSearch={}, maxtime=GOOGLE_SEARCH_MAXTIME):
#     """ Find compounds in PubChem for list of features using google (and in-line update dictionaries)."""
#     HTTPError_status = 0
#     n = len(lst_alias)  # total number of features
#     i = 0
#     starttime = float(time.time())
#     endtime = starttime + maxtime

#     for alias in lst_alias:
#         if time.time() > endtime:
#             break
#         else:
#             # search google
#             google_result, HTTPError_status = _google_pubchemcompound(alias, HTTPError_status=HTTPError_status)
#             compounds, google_identifier = get_pubchemcompound(google_result)  # identifier that matches google search

#             # Update dictionaries
#             if (isinstance(google_result, str)):
#                 alias2google[alias] = google_result
#                 if (google_identifier != ''):
#                     google2googleident[google_result] = google_identifier
#                     identifier = process.extractOne(alias, compounds[0].synonyms, scorer=fuzz.token_sort_ratio)[0]  # identifier that matches original alias
#                     google2ident[google_result] = identifier
#                     if (identifier != '') and (identifier not in list(ident2cid.keys())):
#                         ident2cid[identifier] = [compound.cid for compound in compounds]
#                         for compound in compounds:
#                             cid2compound[compound.cid] = compound
#             else:
#                 FailedGoogleSearch[alias] = google_result
#             if i % 2 == 0:
#                 print(match_method, ' ',  i, 'of', n, '. time left: ' + str(int(endtime - time.time())))
#             i += 1


# def check_pubchemcompound_attrib_lst(cid2compound={}, cid2attrib[cid]={},
#                                      lst_field=['lst_cas', 'cas0', 'lst_name', 'name0']):
#     """ Check for all compounds if the attributes are present."""
#     cids = cid2compound.keys()
#     missing_attrib = []
#     incomplete_attrib = []
#     i = 0
#     for cid in cids:
#         try:
#             # check if data fields exists and with right format
#             if (type(cid2attrib[cid]['lst_cas']) != list):  # KeyError if key does not exist
#                 raise TypeError
#             elif (type(cid2attrib[cid]['cas0']) != str):
#                 raise TypeError
#             elif (type(cid2attrib[cid]['lst_name']) != list):
#                 raise TypeError
#             elif (type(cid2attrib[cid]['name0']) != str):
#                 raise TypeError

#             # check if data fields contain data.
#             field_missing = []
#             for field in lst_field:
#                 if (len(cid2attrib[cid][field]) == 0):
#                     field_missing.append(field)
#                 incomplete_attrib.append(cid)
#         except:
#             missing_attrib.append(cid)
#         if len(missing_attrib) > 0:
#             print('WARNING: PubChem compound and/ or attributes not found for ',  str(n-i), ' of ', str(n))

#     return missing_attrib


#     # step 4: extract missing attribute from compound
#     if check_attrib is False:
#         pass
#     else:
#         n = len(missing_attrib)
#         i = 0
#         missing_attrib2 = []
#         for cid in missing_attrib:
#             try:
#                 cid2attrib[cid] = _get_pubchem_attributes(cid2compound[cid])
#                 if i % 200 == 0:
#                     print('PubChem found missing attrib for compound ',  str(i), ' of ', str(n))
#                 i += 1
#             except:
#                 missing_attrib2.append(cid)
#         if len(missing_attrib2) > 0:
#             print('WARNING: PubChem compound and/ or attributes not found for ',  str(n-i), ' of ', str(n))

#     # step 5: check if attrib is complete (Note: Sometimes Pubchem compound object has no synonyms so empty is not necessarity wrong)
#     if check_attrib is False:
#         pass
#     else:
#         i = 0
#         lst_field = ['lst_cas', 'cas0', 'lst_name', 'name0']
#         for cid in cids:
#             try:
#                 field_missing = []
#                 for field in lst_field:
#                     if (len(cid2attrib[cid][field]) == 0):
#                         field_missing.append(field)
#             except:
#                 field_missing = lst_field
#             if len(field_missing) > 0:
#                 print('WARNING: PubChem fields missing from CID ' + str(cid) + ': ' + str(field_missing))
#                 i += 1
#         if i > 0:
#             print('WARNING: PubChem attributes missing or impcomplete for ',  str(n-i), ' of ', str(n))

#     # TO DO: return ...._not_found, keep other dicts change in_place


# %% DOWNDLOAD

starttime = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
print(starttime)

# Read Excel and pickle
df_alias = pd.read_excel(FILE_EXCEL, sheet_name=SHEET_ALIAS, encoding='ISO-8859-1', skiprows=SKIPROWS).fillna('')

nl2alias = pickle.load(open(FILE_NL2ALIAS, "rb"))
cid2compound = pickle.load(open(FILE_CID2COMPOUND, "rb"))
df_feature_ner = pickle.load(open(FILE_FEATURE_NER, "rb"))
df_cas_ner = pickle.load(open(FILE_CAS_NER, "rb"))


# %% TRANSLATE

# convert all CAS- and Alias-columns to lists of strings
cols = COLS_ALIAS_NL + COLS_ALIAS_EN + COLS_CAS + COLS_DBASE



def to_string(item):
    """Convert item to string; also rounds floats to integers."""
    string = ''
    if isinstance(item, int):
        string = str(int(item))
    if isinstance(item, str)
        string = str(item)
    elif item.is_integer:
        string = str(int(item))
    else:
        string = str(item)

    return string

    for row in range(len(df)):


df = pd.DataFrame({'a':[1, 2., '3', 4.002]})
df['a'].apply(lambda x: to_string(x))
#################################################
        item = df[col][row]
        if isinstance(item, int):
            df[col][row] = str(int(df[col][row]))
        elif item.is_integer():
            df[col][row] = str(int(df[col][row]))
        else:
            df[col][row] = str(df[col][row])
_split_string_in_dfcol_to_list(df=df_alias, cols=cols)

# select Dutch alias to translate
mask = df_alias[COLS_ALIAS_EN].sum(axis=1).apply(lambda c: c == [])  # select rows without English Alias
lst_source = _list_in_df_to_lists(df=df_alias.loc[mask, COLS_ALIAS_NL])  # list Dutch alias in row without English Alias
lst_source2 = list(set(lst_source) - set(nl2alias.keys()))  # select alias not yet translated

# translate Dutch alias to English
lst_dest2 = nersearch.translate_list_by_chunck(lst_source=lst_source2)
nl2alias = {
    **nl2alias,
    **dict(zip(lst_source2, lst_dest2)),  # newly translated terms
}

# update alias df with English translations
mask = df_alias[COLS_ALIAS_EN].sum(axis=1).apply(lambda c: c == [])  # row without English Alias
mask2 = df_alias[COLS_ALIAS_NL].sum(axis=1).apply(lambda c: c == [])  # row without Dutch Alias
df_alias.loc[~mask2 & mask, 'OtherEnglishAlias'] = \
    df_alias[COLS_ALIAS_NL][~mask2 & mask].sum(axis=1).map(lambda x: x[0]).map({k: [v] for k, v in nl2alias.items()})  # get translation of first item om row

end_translate_time = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
print(end_translate_time)


# %% GET PUBCHEM

# Select Alias to retrieve from PubChem
lst_alias = _list_in_df_to_lists(df_alias[COLS_ALIAS_EN])
lst_cas = _list_in_df_to_lists(df_alias[COLS_CAS])

# Retrieve PubChem CID
get_pubchemcompound_feature_lst(
    lst_alias=lst_alias,
    alias2ident=alias2ident,
    ident2cid=ident2cid,
    cid2compound=cid2compound)
get_pubchemcompound_cas_lst(
    lst_cas=lst_cas,
    cas2ident=cas2ident,
    ident2cid=ident2cid,
    cid2compound=cid2compound,
    function_name='cas_pubchem')

# Select Alias to retrieve from PubChem with google
mask = df_alias['FeatureDataBase'] == 'PubChemCompound'  # row that requires PubChem
mask2 = df_alias[COLS_CAS].sum(axis=1).apply(lambda c: c == [])  # row without CAS
lst = _list_in_df_to_lists(df_alias[mask & mask2][COLS_ALIAS_EN])
no_pubchem = [k for k, v in alias2ident.items() if v == '']
lst_alias2google = list(set(lst) & set(no_pubchem))

# Retrieve PubChem CID with google (only selection)
FailedGoogleSearch = {}
_google_pubchemcompound_lst(
    lst_alias2google,
    google2googleident=google2googleident,
    alias2google=alias2google,  # 5 mar 19:00 --> 1103 items
    google2ident=google2ident,  # 924
    ident2cid=ident2cid,
    cid2compound=cid2compound,
    FailedGoogleSearch=FailedGoogleSearch,
    maxtime=GOOGLE_SEARCH_MAXTIME)

# Download missing data
download_missing_pubchem(
    alias2ident=alias2ident,
    cas2ident=cas2ident,
    alias2google=alias2google,
    google2ident=google2ident,
    ident2cid=ident2cid,
    cid2compound=cid2compound
)

get_pubchem_attributes_lst(
    cid2compound=cid2compound,
    cid2attrib=cid2attrib
)

# To do: Add cas2cid for cas that are on webpage but (for unknown reason) not in Compound.synonyms

# save results
pickle.dump(nl2alias, open(FILE_NL2ALIAS, "wb"))  # save result
pickle.dump(alias2ident, open(FILE_ALIAS2IDENT, "wb"))
pickle.dump(cas2ident, open(FILE_CAS2IDENT, "wb"))
pickle.dump(alias2google, open(FILE_ALIAS2GOOGLE, "wb"))
pickle.dump(google2ident, open(FILE_GOOGLE2IDENT, "wb"))
pickle.dump(google2googleident, open(FILE_GOOGLE2GOOGLEIDENT, "wb"))
pickle.dump(ident2cid, open(FILE_IDENT2CID, "wb"))
pickle.dump(cid2compound, open(FILE_CID2COMPOUND, "wb"))
pickle.dump(cid2attrib, open(FILE_CID2ATTRIB, "wb"))


#%% BUILD DICTIONARY FROM ALIAS/CAS TO  CID

di_minscore = {1: 1., 3: 1., 4: .90, 5: .85, 6: .80, 8: .75}
f_minscore = nersearch._interp1d_fill_value(x=di_minscore.keys(), y=di_minscore.values())

# merge all results, 1 row per alias
df0 = pd.DataFrame({'Alias': list(set(lst_alias + list(alias2ident.keys()) + list(alias2google.keys())))})
df1 = pd.DataFrame.from_dict(alias2ident, orient='index').reset_index().rename(columns={'index':'Alias', 0:'Identifier'})
df2 = pd.DataFrame.from_dict(alias2google, orient='index').reset_index().rename(columns={'index':'Alias', 0:'Google'})
df2['IdentifierGoogle'] = df2['Google'].map(google2ident)
df = df0.merge(df1, 'left', on='Alias').merge(df2, 'left', on='Alias')

# clean all data to lowercase ascii
for col in ['Alias', 'Identifier', 'IdentifierGoogle']:
    df = named_entity_recognition.utils._cleanup_alias(df=df,
        col=col,
        col2=col + '2',
        string2whitespace=[],
        string2replace=named_entity_recognition.utils.strings2replace(),
        string2remove=[],  # keep the entire string
        strings_filtered=[])  # not relevant

mask1 = (df['Identifier'].isnull()) | (df['Identifier'] == '')
df['Score'] = np.where(~mask1, df.apply(lambda row: .01 * fuzz.ratio(row['Alias2'], row['Identifier2']), axis=1), 0)
mask2 = (df['IdentifierGoogle'].isnull()) | (df['IdentifierGoogle'] == '')
df['ScoreGoogle'] = np.where(~mask2, df.apply(lambda row: .01 * fuzz.ratio(row['Alias2'], row['IdentifierGoogle2']), axis=1), 0)

mask1 = df['Score'] > df['ScoreGoogle']
mask2 = df[['Score', 'ScoreGoogle']].max(axis=1) >= f_minscore(df['Alias'].astype(str).map(len))
df['BestIdentifier'] = np.where(mask1, df['Identifier'], df['IdentifierGoogle'])
df['PubChemCid'] = np.where(mask2, df['BestIdentifier'].map(ident2cid), '')

alias2cid = dict(zip(df['Alias'], df['PubChemCid']))
cas2cid = {k: ident2cid[v] for k,v in cas2ident.items() if v != ""}


end_pcp_time = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
print(end_pcp_time)

# %% BUILD ALIAS TABLE


def _find_dictvalues(dct, lst):
    try:
        lst = [dct[key] for key in lst]
    except:
        lst = []
    return lst


def _find_and_flatten_dictvalues(dct, lst):
    flat_lst = []
    try:
        nested_lst = [dct[key] for key in lst]
        for item in nested_lst:
            flat_lst += item
    except:
        pass
    return flat_lst


def _find_nested_dictvalues(dct, keys, field):
    try:
        lst = [dct[key][field] for key in keys]
    except:
        lst = []
    return lst


def _find_and_flatten_nested_dictvalues(dct, keys, field):
    flat_lst = []
    try:
        nested_lst = [dct[key] for key in keys]
        for item in nested_lst:
            flat_lst += item[field]
    except:
        pass
    return flat_lst

# map CAS/ Alias columns to CID/ CAS
dct = {
    # 'EnteredCas': [],
    # 'AllAlias': [],
    'Cas2Cid': [],
    'Alias2Cid': [],
    'ValidCas': [],
    'ValidCid': [],
    'BestCas': [],
    'BestCid': [],
    'Warnings': [],
}

for row in range(len(df_alias)):
    # Get CID and CAS for all CAS and Alias columns
    warnings = []
    lst_Cas = df_alias[COLS_CAS].sum(axis=1)[row]  # --> inefficient programming but df not so large
    lst_Cas2Cid = _find_and_flatten_dictvalues(cas2cid, lst_Cas)
    lst_Cas2Cas = _find_and_flatten_nested_dictvalues(cid2attrib, lst_Cas2Cid, 'lst_cas')  # synonyms
    lst_Alias = df_alias[COLS_ALIAS_EN].sum(axis=1)[row]  # --> inefficient programming but df not so large
    lst_Alias2Cid = _find_and_flatten_dictvalues(alias2cid, lst_Alias)
    lst_Alias2Cas = _find_and_flatten_nested_dictvalues(cid2attrib, lst_Alias2Cid, 'lst_cas')

    # select most frequent CID
    all_cid = lst_Cas2Cid + lst_Alias2Cid
    if (len(lst_Cas) > 0) & (len(lst_Cas2Cid) == 0) & (len(lst_Alias2Cid) > 0):  # CAS entered, only CID found for Alias
        shared_cid = []
        warnings.append('CID found for Alias but not for CAS')
        print('WARNING: CID found for Alias but not for CAS: ' + str(lst_Cas) + str(lst_Alias))
    elif (len(lst_Cas) == 0):  # no CAS entered
        shared_cid = list(set(lst_Alias2Cid))
    elif (len(lst_Alias2Cid) == 0):  # no CAS entered
        shared_cid = list(set(lst_Cas2Cid))
    else:
        shared_cid = list(set(lst_Cas2Cid) & set(lst_Alias2Cid))
        if len(shared_cid) == 0:
            shared_cid = list(set(lst_Cas2Cid))  # cas2cid has higher importance than alias2cid
            warnings.append('Alias and CAS both have CID but do not overlap')
            print('WARNING: Alias and CAS both have CID but do not overlap: ' + str(lst_Cas) + str(lst_Alias))
    cid_freq = {x: all_cid.count(x) for x in all_cid if x in shared_cid}  # dictionary with frequency of CID
    if cid_freq == {}:
        cid = []
    else:
        # order from most to least frequent, order by number if frequency is equal
        cid_freq = {k: v for k, v in sorted(cid_freq.items(), key=lambda item: item[0], reverse=False)}  # first order by alphabet (number)
        cid_freq = {k: v for k, v in sorted(cid_freq.items(), key=lambda item: item[1], reverse=True)}  # then order by frequency (overrules previous line)
        cid = list(cid_freq.keys())[0]  # find CID most frequently used. Use first entry in case multiple keys have same frequency.

    # select most frequent CAS (for previsouly selected CID) (Note: in previous loop, we removed CID that conflict with CAS)
    all_cas = list(set(lst_Cas + lst_Cas2Cas + lst_Alias2Cas))
    if cid_freq == {}:
        shared_cas = lst_Cas
    elif (len(lst_Cas) == 0):
        shared_cas = cid2attrib[cid]['lst_cas'] ############################################ for all CID's  ##############################
    else:
        shared_cas = list(set(cid2attrib[cid]['lst_cas']) & set(lst_Cas))
    cas_freq = {x: all_cas.count(x) for x in all_cas if x in shared_cas}
    if cas_freq == {}:
        cas = []
    else:
        cas_freq = {k: v for k, v in sorted(cas_freq.items(), key=lambda item: item[0], reverse=False)}  # first order by alphabet (number)
        cas_freq = {k: v for k, v in sorted(cas_freq.items(), key=lambda item: item[1], reverse=True)}  # then order by frequency (overrules previous line)
        cas = [list(cas_freq.keys())[0]]

    if cid != []:
        cid = [cid]

    # dct['EnteredCas'].append(list(dict.fromkeys(lst_Cas)))  # Entered Cas in original order (unique entries)
    # dct['AllAlias'].append(list(dict.fromkeys(lst_Alias)))  # Entered Alias in original order (unique entries)
    dct['Cas2Cid'].append(lst_Cas2Cid)  # column for QC
    dct['Alias2Cid'].append(lst_Alias2Cid)  # column for QC
    dct['ValidCas'].append(sorted(shared_cas))  # Cas shared by both Alias and Cas input, alphabetically ordered
    dct['ValidCid'].append(sorted(shared_cid))  # Cid shared by both Alias and Cas input, alphabetically ordered
    dct['BestCas'].append(cas)  # most frequent CAS
    dct['BestCid'].append(cid)  # most frequent CID
    dct['Warnings'].append(warnings)  # column for QC

# map CAS/ Alias columns to Cid (and CAS)
df_alias2 = pd.concat([df_alias, pd.DataFrame(dct)], axis=1)
for col in set(COLS_GENERATED) & set(df_alias2.columns):
    df_alias2.loc[(df_alias2['FeatureDataBase'] != 'PubChemCompound'), col] = df_alias2[col].apply(lambda x: [])  # do not overrule 'NrOfRowsMerged' -> later in script

# assign same shared_cid for rows that have same lst_Cas2Cid but empty ('') for lst_Alias2cid
def _sort2string(series):
    return series.apply(lambda x: sorted(x)).apply(lambda x: ','.join(map(str, x)))


df_alias2['Cas2Cid_asstr'] = df_alias2['Cas2Cid'].apply(lambda x: sorted(x)).apply(lambda x: ','.join(map(str, x)))  # sort list on each row, convert to string
df_alias2['Alias2Cid_asstr'] = df_alias2['Alias2Cid'].apply(lambda x: sorted(x)).apply(lambda x: ','.join(map(str, x)))  # sort list on each row, convert to string

df_alias2.sort_values(['Cas2Cid_asstr','Alias2Cid_asstr'], ignore_index=True, ascending=False, inplace=True)  # use ascending so that "" value is last
for row in range(1, len(df_alias2)):
    if (df_alias2['Alias2Cid'][row] == []) & (df_alias2['Cas2Cid'][row] != []) & (df_alias2['Cas2Cid'][row] == df_alias2['Cas2Cid'][row-1]) & (df_alias2['ValidCid'][row] != df_alias2['ValidCid'][row-1]):
        df_alias2['ValidCid'][row] = df_alias2['ValidCid'][row-1]
        df_alias2['Warnings'][row] = df_alias2['Warnings'][row] + ['row merged only based on Cas2Cid because Alias2Cid was not found on all rows']

# assign same shared_cid for rows that have same lst_Alias2Cid but empty ('') lst_Cas2cid
df_alias2.sort_values(['Alias2Cid_asstr','Cas2Cid_asstr'], ignore_index=True, ascending=False, inplace=True)  # use ascending so that "" value is last
for row in range(1, len(df_alias2)):
    if (df_alias2['Cas2Cid'][row] == []) & (df_alias2['Alias2Cid'][row] != []) & (df_alias2['Alias2Cid'][row] == df_alias2['Alias2Cid'][row-1]) & (df_alias2['ValidCid'][row] != df_alias2['ValidCid'][row-1]):
        df_alias2['ValidCid'][row] = df_alias2['ValidCid'][row-1]
        df_alias2['Warnings'][row] = df_alias2['Warnings'][row] + ['row merged only based on Alias2Cid because Cas2Cid was not found on all rows']

# reset and delete unnecessary columns
df_alias2.reset_index(drop=True, inplace=True)
df_alias2.drop(['Alias2Cid', 'Cas2Cid','Cas2Cid_asstr','Alias2Cid_asstr'], axis=1, inplace=True)

# merge rows that have same CID's
sumcols = COLS_GENERATED + COLS_ALIAS_EN + COLS_ALIAS_NL + COLS_DBASE
df_alias2['NrOfRowsMerged'] = 1  # df_alias2['NrOfRowsMerged'].apply(lambda x: [1])

mask = (df_alias2['ValidCid'].apply(lambda x: len(x) > 0)) & (df_alias2['AllowMergeRows'] == True)  # select what rows to merge
by = df_alias2['ValidCid'].apply(lambda x: ','.join(map(str, x)))  # adjust list to string to allow groupby on this column
df_alias3 = df_alias2[mask].groupby(by).agg({'NrOfRowsMerged': 'sum'})  # do this column first -> is integer, not list so cannot be sorted

for col in set(sumcols) - set(df_alias2.columns):
    df_alias3[col] = df_alias2[mask].groupby(by).agg({col: 'sum'})
    df_alias3[col] = df_alias3[col].apply(lambda x: sorted(set(x)) if len(x)>0 else [])
for col in list(set(df_alias2) - set(df_alias3)): # ['DefaultUnits', 'Category', 'FeatureDataBase', 'AllowMergeRows']:
    df_alias3[col] = df_alias2[mask].dropna(subset=[col]).groupby(by).agg({col: 'first'})
df_alias3.reset_index(drop=True, inplace=True)

# merge rows that have same FIRST Alias for rows without CID's (including rows that DO NOT have 'AllowMergeRows' == True)
by = df_alias2[COLS_ALIAS_EN].sum(axis=1).map(lambda x: x[0] if len(x)>0 else '')
df_alias4 = df_alias2[~mask].groupby(by).agg({'NrOfRowsMerged': 'sum'})  # do this column first -> is integer, not list so cannot be sor

for col in set(sumcols) - set(df_alias4.columns):
    df_alias4[col] = df_alias2[~mask].groupby(by).agg({col: 'sum'})
    df_alias4[col] = df_alias4[col].apply(lambda x: sorted(set(x)) if len(x)>0 else [])  # sorted
for col in list(set(df_alias2) - set(df_alias4)): # ['DefaultUnits', 'Category', 'FeatureDataBase', 'AllowMergeRows']:
    df_alias4[col] = df_alias2[~mask].dropna(subset=[col]).groupby(by).agg({col: 'first'})
df_alias4.reset_index(drop=True, inplace=True)

# merge row with and without CID rows
df_alias5 = pd.concat([df_alias3, df_alias4], axis=0)

# merge columns with Alias in same language
df_alias5['AliasEnglish'] = df_alias5[COLS_ALIAS_EN].sum(axis=1).apply(lambda x: list(dict.fromkeys(x)) if len(x)>0 else [])  # maintain original order
df_alias5['AliasDutch'] = df_alias5[COLS_ALIAS_NL].sum(axis=1).apply(lambda x: list(dict.fromkeys(x)) if len(x)>0 else [])  # maintain original order

# define feature
def _cid_lst2string(dfcol):
    """Convert list of CID (integers) to string with prefix. Example: '123' -> 'PubChemCid 123'."""
    return dfcol.apply(lambda x: ['PubChemCid ' + str(item) for item in x]).apply(lambda x: ', '.join(map(str, x)))

df_alias5['Feature'] = np.where(
    (df_alias5['ValidCid'].map(lambda x: len(x)) > 0) & (df_alias5['AllowMergeRows'] == True),  # rows with unique CID that may be merged
    _cid_lst2string(df_alias5['ValidCid']),  # PubChemCompound .....
    df_alias5[COLS_ALIAS_EN].sum(axis=1).map(lambda x: x[0] if len(x)>0 else '')  # first Alias
    )

# export
df_alias5.to_csv(FILE_ALIAS2CSV, encoding='utf-8', index=False)
pickle.dump(df_alias5, open(FILE_ALIAS, "wb"))

end_time = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
print(end_time)


