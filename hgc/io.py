# -*- coding: utf-8 -*-
"""
Routine to read water quality data of different formats and transform to HGC format
Xin Tian, Martin van der Schans, Martin Korevaar
KWR, 2020
Last edit: Nov 6, 2020
"""

import copy
#  import hgc.ner  #################
import logging
import numpy as np
import pandas as pd
import molmass
from pathlib import Path
# from hgc import constants  #################
# from hgc.constants.constants import mw, units_wt_as  #################
from hgc.constants.read_write import load_pickle_as_namedtuples
# from hgc.constants.iodefaults import dict_features_units


################# TO DO
#################  mapping features based on multiple columns (example WML)




# %% Defaults

def default_column_dtype():
    """Generate a dictionary with the default data type for columns often used in HGC."""
    # headers that MUST be defined for HGC
    dct_compulsory = {
        'Feature': 'str',
        'Unit': 'str',
        'Value': 'float64',
    }

    # recommended nomenclator for optional headers
    dct_recommended = {
        'SampleId': 'str',
        'LocationId': 'str',
        'Xcoord': 'float64',
        'Ycoord': 'float64',
        'Zcoord': 'float64',
        'GroundSurface_mMSL' : 'float64',  # relative to Mean Sea Level
        'TopFilter_mBGS': 'float64',  # Below Ground Surface (Dutch: NAP)
        'BottomFilter_mBGS': 'float64',
        'TopFilter_mMSL': 'float64',  # Mean Sea Level (Dutch: NAP)
        'BottomFilter_mMSL': 'float64',
        'Datetime': 'datetime',
        'DetectionLimit': 'float64',
    }
    return {**dct_compulsory, **dct_recommended}


def default_header_map():
    """Generate a mapping for columns often used in HGC, assuming headings remain as-is"""
    dct = dict(zip(default_column_dtype().keys(), default_column_dtype().keys()))
    return dct


def generate_hgcfeatures_units():
    """Generate a dictionary with unit-prefixes (values) for HGC Feature (keys). Example: {'Fe': 'mg/L', 'Al': 'µg/L'}."""
    # extract feature names and get units defined in HGC, for ions, atoms and others
    atoms, ions, properties = load_pickle_as_namedtuples()
    feature = [{**atoms, **ions, **properties}[item].feature for item in {**atoms, **ions, **properties}]
    DefaultUnits = [{**atoms, **ions, **properties}[item].unit for item in {**atoms, **ions, **properties}]
    dct = {k: v for k, v in dict(zip(feature, DefaultUnits)).items() if v is not None}

    return dct


def default_unit_conversion_factor():
    """ Generate a dictionary of conversion factors (values) for several unit-prefixes (keys). Example: {'g/L': 1., 'mg/L': 0.001, 'μg/L': 1e-6}"""
    # units, prefix and conversion factors generated by this function
    units = ['1', 'm', 'g', 'L', 'S', 'V', 'mol']
    prefixes = {'p': 1e-12, 'n': 1e-9, 'μ': 1e-6, 'm': 1e-3, 'c': 0.01, 'k': 1000,
                '100m': 0.1, '50m': 0.05}

    # make a dictionary of all combinations of units/ prefixes (keys) and
    # conversion values (values) {mL: 0.001, L: 1, etc.}
    dct = dict(zip(units, len(units) * [1.]))  # units without prefix get conversion factor "1"
    for unit in units:  # loop through all combinations of units and prefixes
        for key, value in prefixes.items():
            dct[key + unit] = value

    dct = {**dct, '%': .01}  # for correction of percentage

    return dct


def default_na_values():
    """
    Generate list of values that are recognized as NaN.

    The list is based on the default values of python, but with 'NA' left
    out to prevent NA (Sodium) being read as NaN

    Returns
    -------
    List

    """
    lst = ['#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',
           '1.#IND', '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null']
    return lst


# %% main routine: support functions

def read_file(file_path='', sheet_name=0, na_values=[], encoding=None, delimiter=None, **kwargs):
    """
    Read excel of csv file.
    If a file cannot be found, we create an empty file.
    We raise an error to indicate if an empty file is imported.
    """
    file_extension = file_path.split('.')[-1]
    if (file_extension == 'xlsx') or (file_extension == 'xls'):
        try:
            df = pd.read_excel(file_path,
                               sheet_name=sheet_name,
                               header=None,
                               index_col=None,
                               na_values=na_values,
                               keep_default_na=False,
                               )
            # logger.info('file read: ' + file_path)
        except FileNotFoundError:
            raise FileNotFoundError('file not found: {}'.format(file_path))
            # logger.error('WARNING file not read: ' + file_path)
    elif file_extension == 'csv':
        try:
            df = pd.read_csv(file_path,
                             encoding=encoding,
                             header=None,
                             index_col=None,
                             low_memory=False,
                             na_values=na_values,
                             keep_default_na=False,
                             delimiter=delimiter,
                             )
            # logger.info('file read: ' + file_path)
        except FileNotFoundError:
            raise FileNotFoundError('file not found: {}'.format(file_path))
    else:
        raise Exception('file extension not recognized: {}'.format(file_path))
        # logger.error('WARNING file extension not recognized: ' + file_path)

    if df.empty:
        raise Exception('file is read but returns empty dataframe {}'.format(file_path))

    return df


def _get_df_name(df, **kwargs):
    """get the name of dataframe"""
    name = [x for x in globals() if globals()[x] is df]
    return name


def _test_index(df, **kwargs):
    """test if index contains incrementing elements, incremented by one each and starting with 0"""
    lst = df.index
    if len(lst) == 0:
        raise Exception('empty dataframe')
    test = (lst[0] == 0) and (all(y - x == 1 for x, y in zip(lst, lst[1:])))
    if test is False:
        raise Exception('dataframe index must contain 0,1,2,3 etc.')


def _get_slice(df, arrays, **kwargs):
    """Get row values by slicing."""
    if isinstance(arrays[0], list):  # check if the array is nested
        series = pd.Series([], dtype='object')
        for array in arrays:
            series = series.append(df.iloc[array[0], array[1]].rename(0))
    elif len(arrays) == 1:  # only row specified
        series = df.iloc[arrays[0]]
    else:  # row and column specified
        series = df.iloc[arrays[0], arrays[1]]
    return series


def get_headers_stacked(df, slice_header='', **kwargs):
    """Get column headers for a stacked-format dataframe."""
    # create series with headers
    header_sample = _get_slice(df, slice_header)
    # add column names
    ncols = len(df.columns)
    level0 = pd.Series(ncols * [''])
    level0[header_sample.index] = header_sample
    df.columns = level0
    return df, header_sample


def slice_rows_with_values(df, slice_values=None, **kwargs):
    """Get data based on pre-defined slicing blocks."""
    df2 = pd.DataFrame([])
    if isinstance(slice_values[0], list):  # check if the array is nested
        for array in slice_values:
            df2 = pd.concat([df2, df.iloc[array[0]]], axis=0)
    elif len(slice_values) == 2:
        df2 = df.iloc[slice_values[0], slice_values[1]]
    elif len(slice_values) == 1:  # only row specified
        df2 = df.iloc[slice_values[0]]
    return df2

def multilevel_wide_to_stack(df, slice_sampleheader=[], slice_parameters={}, slice_values=[]):
    """ Melt wide format with multilevel header to stacked format."""

    # get dataframe with values -> e.g. 1, <2
    values = slice_rows_with_values(df, slice_values)

    # get dataframe with info about sample -> e.g. Date, Location
    sampleheaders = _get_slice(df, slice_sampleheader).astype(str)
    samples = df.iloc[values.index, sampleheaders.index]
    samples.columns=list(sampleheaders)

    # get dataframe with parameters -> e.g. units, features, cas
    parameters = pd.DataFrame([])
    for key, value in slice_parameters.items():
        parameters = pd.concat([parameters, pd.DataFrame({key: _get_slice(df, value).astype(str)}).T], axis=0)

    # generate a list of dataframes -> 1 dataframe per parameter/ column
    lst_values = []
    for col in list(parameters):
        value = pd.DataFrame({'Value': values[col]})
        for index in parameters.index:
            value[index] = parameters.loc[index, col]
        lst_values.append(pd.concat([samples, value], axis=1))

    # concatenate the list of dataframes (concatenating every loop would costs a lot of extra time)
    df_stack = pd.concat(lst_values, axis=0, ignore_index = True)

    return df_stack


def cleanup_headers_stacked(df, header_map={}, **kwargs):
    """remove columns that are blank, duplicate or will be duplicate after mapping for stacked format."""
    # remove duplicate columns, only keep the column that shows for the first time!
    df = df.loc[:,~df.columns.duplicated()]
    # remove columns without headers
    mask1 = df.columns.get_level_values(0).isin([''] + list(np.arange(0, len(df.columns))))
    cols1 = np.array(list(df))[mask1]
    cols2 = list(cols1)
    df.drop(cols2, axis=1, inplace=True)
    # remove columns that are in values but absent from keys
    # (to prevent identical column names when mapping column names)
    keys = header_map.keys()
    values = header_map.values()
    col_in_val_not_in_key = list(set(df.columns) & set(values) - set(keys))
    df.drop(col_in_val_not_in_key, axis=1, inplace=True)
    return df


def map_headers(df, header_map={}, **kwargs):
    """rename headers based on mapping of header names."""
    # make a list of headers that are mapped and not mapped
    mapped_headers_before = list(set(header_map.keys()) & set(df.columns))
    mapped_headers_after = list(map(header_map.get, mapped_headers_before))
    unmapped_headers = list(set(df.columns) - set(mapped_headers_before))
    # rename columns in df
    df.rename(columns=header_map, inplace=True)
    return df



def map_features(df, feature_map={}, **kwargs):
    """Map feature names according to pre-defined dictionary."""
    # make a list of features that are mapped
    features_before = list(set(feature_map.keys()) & set(df['Feature']))
    features_after = list(map(feature_map.get, features_before))
    unmapped_features = list(set(df['Feature']) - set(features_before))
    # rename features in df
    df['Feature_orig'] = df['Feature']
    df['Feature'] = df['Feature'].map(feature_map).fillna(df['Feature']) ################ keep old col ??????
    # flag rows with incorrect mapping
    flag = ~df['Feature_orig'].isin(feature_map.keys())
    df.loc[flag, '_Logging_'] = df['_Logging_'] + 'map_features()' + ' | '

    return df


def map_original_units(df, unit_map={}, **kwargs):
    """Map original units according to pre-defined dictionary."""
    # make a list of units that are mapped
    units_before = list(set(unit_map.keys()) & set(df['Unit']))
    units_after = list(map(unit_map.get, units_before))
    unmapped_units = list(set(df['Unit']) - set(units_before))
    # rename original units to units recognized by hgc
    df['Unit_orig'] = df['Unit']
    df['Unit_orig0'] = df['Unit_orig'].map(unit_map).fillna(df['Unit_orig'])
    # flag rows with incorrect mapping
    flag = ~df['Unit_orig'].isin(unit_map.keys())
    df.loc[flag, '_Logging_'] = df['_Logging_'] + 'map_original_units()' + ' | '
    return df


def map_new_units(df, features_units={}, **kwargs):
    """Map new units (used by hgc) according to a pre-defined dictionary."""
    df['Unit'] = df['Feature'].map(features_units).fillna(df['Unit'])
    # flag rows with incorrect mapping
    flag = ~df['Feature'].isin(features_units.keys())
    df.loc[flag, '_Logging_'] = df['_Logging_'] + 'map_new_units()' + ' | '

    return df


def unit_conversion_ratio(df, unit_conversion_factor={}, features_units={}, **kwargs):
    """Compute conversion factor between original units and new units (used by hgc)."""
    # generate a temporary dataframe to compute ratios
    df2 = pd.DataFrame()

    # split units into inidivual symbols (incl. prefix) and fill NaN
    df2[['orig1', 'orig2', 'orig3']] = df['Unit_orig0'].fillna('').str.split(r"/| ", expand=True, n=2).reindex(columns=range(3)).copy()
    df2[['new1', 'new2', 'new3']] = df['Unit'].fillna('').str.split(r"/| ", expand=True, n=2).reindex(columns=range(3)).copy()
    df2.loc[:, 'unit_conversion_correct'] = True

    for orig, new in {'orig1': 'new1', 'orig2': 'new2'}.items():
        # force column to string (for example 1/m --> '1', 'm')
        df2[orig] = df2[orig].fillna('').astype(str)
        df2[new] = df2[new].fillna('').astype(str)

        # replace non-dimensional units by "1"
        df2[orig] = df2[orig].replace(['n', '-', '', ' '], '1').astype(str)
        df2[new] = df2[new].replace(['n', '-', '', ' '], '1').astype(str)

        # find where to correct for "mol"
        maska = df2[orig].str.contains("mol")
        maskb = df2[new].str.contains("mol")
        mask = maska | maskb

        # compute ratio mol --> gram
        features = df['Feature'][mask].unique()
        map_molwt = {}
        for feature in features:
            try:
                map_molwt[feature] = molmass.Formula(feature).mass
            except:  # in case feature is not a chemical formula
                map_molwt[feature] = np.nan

        df2[orig + 'ratio'] = np.where(maska, df['Feature'].map(map_molwt), 1.)
        df2[new + 'ratio'] = np.where(maskb, df['Feature'].map(map_molwt), 1.)

        # flag errors
        df2.loc[df2[orig + 'ratio'].isnull(), 'unit_conversion_correct'] = False
        df2.loc[df2[new + 'ratio'].isnull(), 'unit_conversion_correct'] = False

        # replace mol by gram (to prevent error in next step) and remove N, P, S
        df2[orig] = df2[orig].str.replace('mol', 'g')
        df2[new] = df2[new].str.replace('mol', 'g')
        df2.loc[maska, 'orig3'] = ''
        df2.loc[maskb, 'new3'] = ''

        # compute unit conversion ratio
        df2[orig + 'ratio'] = df2[orig + 'ratio'] * df2[orig].map(unit_conversion_factor)
        df2[new + 'ratio'] = df2[new + 'ratio'] * df2[new].map(unit_conversion_factor)

        # flag errors. If orig and new both np.nan --> replace ratio's by 1
        df2.loc[df2[orig + 'ratio'].isnull() | df2[new + 'ratio'].isnull(), 'unit_conversion_correct'] = False
        df2.loc[(df2[orig] == '') & (df2[new] == ''), [orig + 'ratio', new + 'ratio']] = 1.

        # there is no error if the units are on purpose empty (eg ph)
        df2.loc[(df['Unit'] == '1') & (df['Unit_orig0'] == '1'), 'unit_conversion_correct'] = True

    # force column to string
    df2['orig3'] = df2['orig3'].fillna('').astype(str)
    df2['new3'] = df2['new3'].fillna('').astype(str)

    # find where to compute N, P, S, correction factor (for example mg/L N --> mg/L)
    mask3a = df2['orig3'] != ''
    mask3b = df2['new3'] != ''
    mask3 = mask3a | mask3b

    df2.loc[mask3 & ~mask3a, 'orig3'] = df['Feature']
    df2.loc[mask3 & ~mask3b, 'new3'] = df['Feature']

    # compute weight of atoms/ features
    atoms = pd.unique(df2[['orig3', 'new3']][mask3].values.ravel())
    map_molwt3 = {}
    for atom in atoms:
        if atom == 'PO4_ortho':
            atom = 'PO4'
        try:
            map_molwt3[atom] = molmass.Formula(atom).mass
        except:  # in case atom is not a chemical formula
            map_molwt3[atom] = np.nan

    df2['orig3ratio'] = np.where(mask3, df2['orig3'].map(map_molwt3), 1.)
    df2['new3ratio'] = np.where(mask3, df2['new3'].map(map_molwt3), 1.)

    # flag errors where conversion factor was not computed
    df2.loc[df2['orig3ratio'].isnull(), 'unit_conversion_correct'] = False
    df2.loc[df2['new3ratio'].isnull(), 'unit_conversion_correct'] = False

    flag = df2['unit_conversion_correct']==False
    df.loc[flag, '_Logging_'] = df['_Logging_'] + 'unit_conversion_ratio()' + ' | '  # flag df based on df2

    # Compute ratio, set ratio to 1 if original and desired unit are equal
    df['Ratio'] = ((df2['orig1ratio'] / df2['new1ratio']) * (df2['new2ratio'] / df2['orig2ratio']) * (df2['new3ratio'] / df2['orig3ratio']))
    df['Ratio'] = np.where((df['Unit_orig'] == df['Unit']) | (df['Unit_orig0'] == df['Unit']), 1., df['Ratio'])

    return df


def split_detectionlimit_from_value(df, **kwargs):
    """
    Split strings ("<", ">") from "Values" and "DetectionLimit" (optional).
    Split ("-") only if the Unit columns contains a concentration "g/L".

    Put string symbol and value in separate columns.
    Only works if string is at the left most position.
    For example: "<100" --> "<" in temporary column, "100" remain in value column.

    Add column "DetectionLimit_orig" if detectionlimit is entered in separate column.
    Merge "<" with the "DetectionLimit", if the Detectionlimit is not specified in Value column.

    Parameters
    ----------
    df: dataframe
        df must contain a column 'Value' and 'Unit'

    """
    df['Value_orig'] = df['Value']
    df['Value'] = df['Value'].astype('str', errors='ignore')

    # select rows that start with < or >
    symb_list = ['<', '>']
    left_symbol = df['Value'].str[0]
    mask = left_symbol.isin(symb_list)
    df['Value_sign'] = np.where(mask, left_symbol, np.nan)
    df['Value_num'] = np.where(mask, df['Value'].str[1:], df['Value'])

    # # select rows that start with '-' and contain concentrations data (other data is allowed <0)
    # mask1 = (pd.to_numeric(df['Value'], errors='coerce') < 0) & (df['Unit'].str.contains('g/L'))
    # df['Value_sign'] = np.where(mask1, '-', df['Value_sign'])
    # df['Value_num'] = np.where(mask1, -pd.to_numeric(df['Value'], errors='coerce'), df['Value_num'])

    # store original detectionlimit in separate column
    if 'DetectionLimit' in df.columns:
        df['DetectionLimit_orig'] = df['DetectionLimit']

        # Merge "<" with the "DetectionLimit", if the Detectionlimit is not specified in Value column.
        mask2 = df['Value_num'].isnull() | (df['Value_num'] == '')
        mask3 = df['Value_sign'].isin(symb_list)
        df['Value_num'] = np.where(mask2 & mask3, df['DetectionLimit'], df['Value_num'])

    return df


def _convert_dtype(series, dtype='', dayfirst=None, **kwargs):
    if dtype == 'datetime':
        series1 = pd.to_datetime(series, dayfirst=dayfirst, errors='coerce')
    elif dtype == 'xlsdatetime':
        mask = pd.to_datetime(series, dayfirst=dayfirst, errors='coerce').isnull()
        series1 = np.where(mask, pd.to_datetime(pd.to_numeric(series, downcast='float', errors='coerce'), unit='D', origin='1899-12-30'), pd.to_datetime(series, dayfirst=dayfirst, errors='coerce'))
    elif dtype == 'float64':
        series1 = pd.to_numeric(series, errors='coerce')
    elif dtype == 'float':
        series1 = pd.to_numeric(series, downcast='float', errors='coerce')
    elif dtype in ['int', 'str']:
        series1 = series.astype(dtype, errors='ignore')
    else:
        raise Exception('ERROR: dtype format not recognized :' + str(dtype))
    return series1


def adjust_dtype(df, column_dtype={}, dayfirst=None, **kwargs):
    """Convert column to userdefined datatype.

    Parameters
    ----------
    df : dataframe
    column_dtype : dictionary
        Mapping between column name (keys) and datatype (values).
        supported dtypes (values) are: 'str', 'float', 'float64', 'datetime', 'int'
    dayfirst : string
        Use dayfirst when converting dates (see pd.datetime documentation)

    """
    # assign dtype for column "Value" to "Value_num".
    column_dtype = {**column_dtype, 'Value_num': column_dtype['Value']}
    # column_dtype.pop('Value', None)
    # if 'DetectionLimit' in df.columns:
    #     column_dtype.pop('DetectionLimit', None)
    # adjust formats
    for col in df.columns:
        if col in column_dtype.keys():
            df[col] = _convert_dtype(df[col], dtype=column_dtype[col], dayfirst=dayfirst)

    return df


def unit_conversion_value(df, **kwargs):
    """Multiply Value (and DetectionLimit) by conversion ratio."""
    df['Value_num'] = df['Value_num'] * df['Ratio']
    if 'DetectionLimit' in df.columns:
        df['DetectionLimit'] = df['DetectionLimit'] * df['Ratio']
    return df


def drop_nan_value_rows(df, **kwargs):
    """Keep rows with numeric Value, '<' or '>' (even if detectionlimit not specified in "Value" column)."""
    # df.dropna(subset=['Value_num'], inplace=True)  # short version
    mask = df['Value_num'].isnull()  # rows without value
    mask2 = df['Value_sign'].isnull()  # rows without "<" or ">"
    df_dropna = df[mask & mask2]
    df = df[~mask | ~mask2]
    return df, df_dropna


def join_detectionlimit_to_value(df, **kwargs):
    """Put sign and numeric value together. For example: "<" + "100" = "<100")."""
    mask = df['Value_sign'].isnull()
    df['Value'] = np.where(mask, df['Value_num'], df['Value_sign'] + df['Value_num'].astype(str))
    # update detectionlimit with Value
    if 'DetectionLimit' in df.columns:
        df['DetectionLimit'] = np.where(~mask & (df['DetectionLimit'].isnull()), df['Value_num'], df['DetectionLimit'] )
    return df


def delete_duplicate_rows(df, column_drop_duplicates=[], **kwargs):
    """Drop rows that have identical values in user defined columns (column_drop_duplicates)."""
    if (isinstance(column_drop_duplicates, list)) and (len(column_drop_duplicates) > 0):
        df_keep = df.drop_duplicates(subset=column_drop_duplicates, keep='first')
        df_dropduplicate = df[~df.index.isin(df_keep.index)]
    else:
        df_keep = df
        df_dropduplicate = pd.DataFrame([])
    return df_keep, df_dropduplicate



# %% main routine


def read_stacked(dataframe=None,
                 file_path='',
                 sheet_name=0,
                 slice_header=[0],
                 slice_values=[slice(1, None)],
                 header_map=default_header_map(),
                 encoding='ISO-8859-1',
                 delimiter=None,
                 na_values=default_na_values(),
                 **kwargs):
    """
    Read and convert stacked data (CSV, XLSX, pd.DataFrame) to meet requirements for "hgc.io.convert_data()".

    Parameters
    ----------
    dataframe : dataframe (optional)
        Dataframe containing the data to be transformed to HGC.
        The headers should be inside the data (and NOT defined as column label)
        "Dataframe" is optional. If empty, the file referred to by file_path will be read.
    file_path: string (optional)
        The path of the excel or csv-file to read.
        Either "dataframe" or "file_path" should be defined.
    sheet_name: string, integer
        The name (string) or number (integer) of the sheet to read.
        Defaults to 0: 1st sheet as a DataFrame
        Only used when reading excel file.
    slice_header: list/ slice
        indicate rows where to find column headers.
        example: [0] -> row 0;
    slice_values: list/ slice,
        indicate rows where to find data (observations).
        example: [slice(2, 5)] -> row 2 to 5
                 [slice(2, none)] -> row 2 to end of file
    header_map: dict,
        Mapping of headers in original file (keys) to headers used by HGC (values).
        Default is generated by function default_header_map()
    encoding : string (optional)
        Encoding used when reading csv (see pandas.read_csv for definition)
        Default is 'ISO-8859-1'
    na_values : list (optional)
        List of values that are recognizes as NaN.
        Default is a custom list geneated by the function "default_na_values()".
        This list does not recognize "NA" as NaN value and thus helps to prevent
        misinterpretation of "NA" as "Sodium" (=Na).
    delimiter : string (optional)
        Delimeter used when reading csv (see pandas.read_csv for definition)
        Default is 'None'

    Returns
    -------
    df : Dataframe
        Stacked dataframe that meets requirements for "hgc.io.convert_data()"

    """
    # generate a dictionary with all arguments passed in the function
    arguments = copy.deepcopy(locals())

    # read dataframe
    df_read = pd.DataFrame([])
    if isinstance(dataframe, pd.DataFrame):
        print('start read dataframe: ' + str(_get_df_name(dataframe)))
        df_read = dataframe.reset_index(drop=True)  # copy to prevent change to initial df and reset index
    elif isinstance(file_path, str):
        print('start read file: ' + file_path)
        df_read = read_file(file_path=file_path, sheet_name=sheet_name,
            na_values=na_values, encoding=encoding, delimiter=delimiter)

    # # convert int to str (to prevent error when combining with np.nan)
    # df_read = df_read.applymap(str)

    # reshape data and headers
    df, header_sample = get_headers_stacked(df_read, slice_header=slice_header)
    df = slice_rows_with_values(df, slice_values=slice_values)
    df = cleanup_headers_stacked(df, header_map=header_map)
    df.reset_index(drop=True, inplace=True)
    df = map_headers(df, header_map=header_map)

    return df


def read_wide(dataframe=None, file_path='',
              sheet_name=0,
              slice_sampleheader=[],
              slice_parameters={},
              slice_values=[slice(1, None)],
              header_map=default_header_map(),
              encoding='ISO-8859-1',
              delimiter=None,
              na_values=default_na_values(),
              **kwargs):
    """
    Read and convert stacked data (CSV, XLSX, pd.DataFrame) to meet requirements for "hgc.io.convert_data()".

    Parameters
    ----------
    dataframe : dataframe (optional)
        Dataframe containing the data to be transformed to HGC.
        The headers should be inside the data (and NOT defined as column label)
        "Dataframe" is optional. If empty, the file referred to by file_path will be read.
    file_path: string (optional)
        The path of the excel or csv-file to read.
        Either "dataframe" or "file_path" should be defined.
    sheet_name: string, integer
        The name (string) or number (integer) of the sheet to read.
        Default: 0 (=1st sheet of Excel)
        Only used when reading excel file.
    slice_sampleheader: nested slice
        Indicate where to find the headers that define the sample such as
        Location, SampleId, Date (but excluding Feature and Units).
        example: [0, slice(0, 5)] -> row 0, column 0-5
    slice_parameters: dictionary {parameter: slice}
        Indicate where to find the Features/ Units (column names in wide dataframe)
        example: {'Feature': [0, slice(5, 20)],  # -> Features are in row 0, column 5 -20
                  'Unit': [1, slice(5, 20)]      # -> Units are in row 1, column 5 -20
    slice_values: nested slice
        indicate rows where to find data (observations).
        example: [slice(2, 5)] -> row 2 to 5
                 [slice(2, none)] -> row 2 to end of file
    header_map: dict,
        Mapping of headers in original file (keys) to headers used by HGC (values).
        Default is generated by function default_header_map()
    encoding : string (optional)
        Encoding used when reading csv (see pandas.read_csv for definition)
        Default is 'ISO-8859-1'
    na_values : list (optional)
        List of values that are recognizes as NaN.
        Default is a custom list geneated by the function "default_na_values()".
        This list does not recognize "NA" as NaN value and thus helps to prevent
        misinterpretation of "NA" as "Sodium" (=Na).
    delimiter : string (optional)
        Delimeter used when reading csv (see pandas.read_csv for definition)
        Default is 'None'

    Returns
    -------
    df : Dataframe
        Stacked dataframe that meets requirements for "hgc.io.convert_data()"

    Notes
    -----
    When features/ units are entered duplicate, the left most column is retained.

    """
    # read dataframe
    df_read = pd.DataFrame([])
    if isinstance(dataframe, pd.DataFrame):
        print('start read dataframe: ' + str(_get_df_name(dataframe)))
        df_read = copy.deepcopy(dataframe)  # deep copy to prevent changing df
        if _test_index(df_read) == False:
            print('index does not contain incrementing numbers 0,1,2,3,etc.')
    elif isinstance(file_path, str):
        print('start read file:' + file_path)
        df_read = read_file(file_path=file_path, sheet_name=sheet_name,
            na_values=na_values, encoding=encoding, delimiter=delimiter)

    # # convert int to str (to prevent error when combining with np.nan)
    # df_read = df_read.applymap(str)

    # reshape data and headers
    df = multilevel_wide_to_stack(df_read, slice_sampleheader=slice_sampleheader,
        slice_parameters=slice_parameters, slice_values=slice_values)
    df = cleanup_headers_stacked(df, header_map=header_map)
    df = map_headers(df, header_map=header_map)

    return df


def read_gef(file_path='', folder_path='', data_model='NL_DINOBRO_2020'):
    """
    Read Geofysical Exchange Formats to meet requirements for "hgc.io.convert_data()"

    (currenlty only implemented for DINO-BRO, Netherlands)

    Parameters
    ----------
     file_path: string (optional)
        The path of the gef-file to read.
        Either "folder_path" or "file_path" should be defined.
     folder_path: string (optional)
        The folder (including subfolders) to read all gef-file.
        Either "folder_path" or "file_path" should be defined.
     data_model: string
         default: 'NL_DINOBRO_2020'
         no other options are currently available

    Return
    ------
    df : pd.dataframe
        Stacked dataframe that meets requirements for "hgc.io.convert_data()"

    """
    # to be implemented
    print('under construction')

    return None


def harmonize(df_read=pd.DataFrame([]),
              symbol_map={},
              feature_map={},
              unit_map={},
              unit_conversion_factor=default_unit_conversion_factor(),
              features_units={# **dict_features_units(),############################################
                             **generate_hgcfeatures_units()},
              column_dtype=default_column_dtype(),
              column_drop_duplicates=['Feature', 'SampleId'],
              dayfirst=True,
              **kwargs):
    """
    Harmonize the features, units and values of dataframe with stacked shaped data and dtype of other columns.

    Parameters
    ----------
    dataframe : dataframe
        Stacked format
        Headers: SampleId, Feature, Value, Unit
    symbol_map : dict, optional
        Mapping of which symbols to replace (per column).
        Syntax "{<Column> : {<pat>: <repl>}}".
        <pat> and <repl> are arguments of pandas.Series.str.replace.
        Example replace detection limit symbol:
            "{'Value': {'NA': '<'}}" -> replaces 'NA' by '<' in column 'Value'
        Example to replace the decimal symbol (default is ".")
            "{'DetectionLimit':, {',': ','}}" -> replaces komma by point in column 'DetectionLimit'.
    feature_map : dict
        Mapping of original features (keys) to features used by HGC (values).
    unit_map : dict, optional
        Mapping of original units (keys) to units recognized by this import script (values).
        ==> Note 1: the values should be defined in SI units and note they are case sensitive.
        The values may contain units that are recognized by the script:
        mol, g, m, h, min, L, V, C, S, Bq, pve, kve, pvd, fte, n
        ==> Note 2: The values may also contain all symbols for conversion factors that are
        defined by the "unit_conversion_factor" argument (see below). e.g. μmol/L.
        ==> Note 3: the following formats can be handled:
        <prefix, optional> <units> e.g. "Bq" or "kBq"
        <prefix, optional> <units> <backslash> <prefix, optional> <units> e.g. "1/m", "mg/nL"
        <prefix, optional> <units> <backslash> <prefix, optional> <units> <whitespace> <atom> e.g. "mg/L N"
        ==> Note 4: The import script can handle "concentrations as"
        (e.g. NO3 mg-N/L, PO4 mg-P/L). They are automatically
        converted to "concentrations" (e.g. (NO3) mg/L) in subsequent steps of this
        import procedure by correcting for the molar weight difference.
        ==> Note 5: "concentrations as" should be mapped to a format with the
        "as" atom behind the units, separated by a whitespace (e.g. NO3 mg/L N,
        PO4 mg/L P). A corresponding dictionary should then be defined as follows for mapping:
        {mg-N/L: mg/L N, mg-P/L: mg/L P} (see also Note 3).
    unit_conversion_factor : dict (optional)
        to convert units of different magnitudes.
        Default is a dictionary of frequently used waterquality units that are
        generated by the function "default_unit_conversion_factor()"
    features_units : dict (optional)
        Mapping between features (keys) and the corresponding units (units).
        To be used in case the units are not defined in the input file.
        Default is a combination of the dictionary based on the units used by HGC and
        and the dictionary generated by the function "default_features_units()".
    column_dtype : dict, optional
        Dictionary defining for each column name (keys) what datatype to use (values).
        supported dtypes: 'datetime', 'xlsdatetime', 'float64', 'float', 'int', 'str'.
        Default is generated by the function "default_column_dtype()"
        'xlsdatetime' -> first checks if the date is written as excel date (start in 1899)
        and else use pd.DateTime.
    column_drop_duplicates : list
        list of columns that are used to identify duplicate rows (data entered twice)
        (e.g.: if the Feature 'NO3' is entered twice under SampleId 'XX-12345', then only the first entry is kept)
        IF "[]" -> skip identification of duplicate rows (no data is dropped)
        default: ['Feature', 'SampleId']
    dayfirst : Bool (optional)
        Specify a date parse order for pandas.to_datetime().
        Default is True.


    Notes
    -----
    The slice arguments ("slice_header", "slice_feature", "slice_unit", "slice_values")
    refer to certain rows and/or columns of the imported table.
    Example: "[9, slice(2, 5)]" refers to column 2 to 5 of row 9.
    In this function, slice does not work  with ":" symbol. Instead use "None" for infinitive.
    Example: to select the entire row 9 except the first 2 columns use "[9, slice(2, None)]"

    When features/ units are entered duplicate, the behaviour depends on the data shape:
    - with stacked shaped data, only the first entry is kept.
    - with wide shape data, the feature/ unit that comes first in alphabetic order
    is kept. Note that they are ordered based on the original feature/ unit (i.e. before mapping).


    Returns
    -------
    df : dataframe
        == COLUMNS ENTERED BY USER ==
        column: 'SampleId' (optional)
        column: 'Feature'
        column: 'Value'
        column: 'Unit'
        column: 'DetectionLimit': (optional)
            Required for values below detectionlimit in case detectionlimit is not specified in the value column
        == NEW COLUMNS GENERATED BY FUNCTION ==
        column: 'Feature_orig':
            Feature in the original dataframe
        column: 'Unit_orig':
            Units in the original dataframe
        column: 'Unit_orig0':
            Units in the original dataframe, after conversion to HGC compatible units
        column: 'Value_orig'
            Value before correcting for conversion from 'Unit_orig' to 'Unit'
        column: 'DetectionLimit_orig': (optional)
            Detection limit in the original dataframe
        column: 'Ratio'
            Conversion factor between 'Value_orig' and 'Value'
            (based on difference between 'Unit' and 'Unit_orig')
        column: 'Value_num'
            Value, without "<" symbol
        column: 'value_sign'
            "<" symbol if below detectionlimit
       column:  '_Logging_':
            flags the name of the child-function where errors occurred in mapping features and units.
            For example, "map_original_features()" is raised for features in the original file
            that are not included in the 'feature_map' argument.
    dct: dictionary with information on dropped data
        df_dropna:
            dataframe with all NA-entries
        df_dropduplicates:
            dataframe with duplicate entries

    """
    # check if essential columns are present
    for col in ['Feature', 'Value', 'Unit'] + column_drop_duplicates + list(symbol_map.keys()):
        try:
            df_read[col]
        except:
            raise Exception('ERROR. column missing :' + col)

    # check if there is no overlap with columns generated by the function
    newcols = ['Feature_orig', 'Unit_orig', 'Unit_orig0', 'Value_orig', 'Value_num', 'Value_sign', 'Ratio', '_Logging_']
    for col in newcols:
        if col in df_read.columns:
            print('WARNING: column already exists -> deleted: ' + col)

    # deep copy to prevent accidental adjustment of original df
    df = copy.deepcopy(df_read)
    df['_Logging_'] = ''

    # map features
    df = map_features(df, feature_map=feature_map)
    # map units
    df = map_original_units(df, unit_map=unit_map)
    df = map_new_units(df, features_units=features_units)
    # compute ratio to convert units
    df = unit_conversion_ratio(df, unit_conversion_factor=unit_conversion_factor, features_units=features_units)
    # values: split string containing < or >. Add detection limit if stored in a separate column
    df = split_detectionlimit_from_value(df)
    # adjust format for each column based on a user-defined format
    df = adjust_dtype(df, column_dtype=column_dtype, dayfirst=dayfirst)
    # multiply values by the previously computed unit conversion ratio
    df = unit_conversion_value(df)
    # join columns of symbols and values
    df = join_detectionlimit_to_value(df)
    # drop rows with nan values
    df, df_dropna = drop_nan_value_rows(df)
    # delete duplicate rows
    df, df_dropduplicates = delete_duplicate_rows(df, column_drop_duplicates=column_drop_duplicates)
    # sort the column names
    df.sort_index(axis=1, ascending=True, inplace=True)

    # create dictionary with information on dropped data
    dct = {
        'df_dropna': df_dropna,
        'df_dropduplicates': df_dropduplicates
    }

    return df, dct


def stack_to_hgc(df_harmonized=pd.DataFrame([]), index=['LocationId', 'Datetime', 'SampleId'], **kwargs):
    """
    Pivot stacked dataframe to wide format used by HGC.

    Parameters
    ----------
    df_harmonized : Dataframe
        Return of hgc.io.harmonize()
    index : list
        columns to use as index when pivoting data
        Default = ['LocationId', 'Datetime', 'SampleId']

    Return
    ------
    Dataframe

    """
    for col in ['Feature', 'Value'] + index:
        try:
            df_harmonized[col]
        except:
            raise Exception('ERROR. column missing: ' + col)

    df = df_harmonized.pivot_table(index=list(set(df_harmonized.columns) & set(index)),
                                   columns='Feature', values='Value', aggfunc='first')
    return df


# %% convenience routine: support functions

def assign_column(df, newcol_value_map={}, **kwargs):
    """ Create a new column with constant value.

     Parameters
    ----------
    df: pandas.DataFrame
    newcol_value_map: dictionary
        Mapping between name of new column (keys) and the constant value (values).
        Example: {'Owner': 'NYcity'} -> assing the value 'NYcity' to new column 'Owner'
    """
    for newcol, value in newcol_value_map.items():
        df[newcol] = value
    return df


def map_symbols(df, symbol_map={}, **kwargs):
    """Replace symbols in selected columns based on mapping of old and new symbols.

    Parameters
    ----------
    df: Dataframe
    symbol_map: dict
        key = old string. eDfine keys as literal if they contain regex symbols. For example -> re.escape(' (P)')
        value = new string

    NOTE: mapping integers leads to np.nan -> first convert to float (ignoring non-numeric values)!
    """
    for column, value in symbol_map.items():
        df[column] = pd.to_numeric(df[column], errors='ignore', downcast='float')  # ################################### Or convert to text -> OutOfMemory error
        for pat, repl in value.items():
            df[column] = df[column].str.replace(pat, repl)
    return df


def find_substring(string, position= 'between', left_sub='', right_sub='',
                 rindex_left_sub=False, rindex_right_sub=False,
                 return_substring=False, except_value=True, **kwargs):
    """ Get string between, outside, before or after a substring.

    Parameters
    ----------
    string: str
    position: st
        'between' (default) -> string between substring.
        'oustide'  -> string outside substring.
        'before' -> string before substring (based on parameter 'right_sub')
        'after' ->  string after substring  (based on parameter 'left_sub')
        'entire' -> return entire string
    left_sub: str
        left substring.
    right_sub: str
        right substring.
    rindex_left_sub: bool
        False (default) -> find left_sub orrurrence for left substring (left_sub).
        True -> find right_sub occurrence for left substring (left_sub).
    rindex_right_sub: bool
        same as rindex_left_sub, but for right substring (right_sub).
    return_substring: bool
        True -> include the left_sub and right_sub in the return string
    except_value: bool, str
        True -> return original string when exception is raised (e.g. symbol not found)
        str -> return except_value

    Return
    ------
    partial string

    """
    lenleft_sub = len(left_sub)
    lenright_sub = len(right_sub)
    try:
        if position is 'entire':
            string2 = string
        elif position is 'before':
            start = 0
            if rindex_right_sub == False:
                end = string.index(right_sub, start)
            elif rindex_right_sub == True:
                end = string.rindex(right_sub, start)
            if return_substring is True:
                end = end + lenright_sub
            string2 = string[:end]
        elif position is 'after':
            if rindex_left_sub == False:
                start = string.index(left_sub) + len(left_sub)
            elif rindex_left_sub == True:
                start = string.rindex(left_sub) + len(left_sub)
            if return_substring is True:
                start = start - lenleft_sub
            string2 = string[start:]
        else:
            if rindex_left_sub == False:
                start = string.index(left_sub) + len(left_sub)
            elif rindex_left_sub == True:
                start = string.rindex(left_sub) + len(left_sub)
            if rindex_right_sub == False:
                end = string.index(right_sub, start)
            elif rindex_right_sub == True:
                end = string.rindex(right_sub, start)
            if position is 'between':
                if return_substring is True:
                    start = start - lenleft_sub
                    end = end + lenright_sub
                string2 = string[start:end]
            elif position is 'outside':
                if return_substring is False:
                    start = start - lenleft_sub
                    end = end + lenright_sub
                string2 = string[0:start] + string[end:]
    except:
        if except_value is True:
            string2 = string
        elif isinstance(except_value, str):
            string2 = except_value

    return string2


def map_column_by_substring(df, newcol_substring_map={}, **kwargs):
    """ Convenience function to map column of dataframe (or series) to a new column (or series) by substring.
        Note: for kwagrs (optional parameters), see function find_substring()."""
    for newcol, args in newcol_substring_map.items():
        df[newcol] = df[args['orig_col']].apply(lambda x: find_substring(x, **args))
    return df


def map_column_by_dict(df, oldcol='', newcol='', column_by_dict={}, missing_values=True):
    """ Map a dataframe column to a new dataframe, with a dictionary.

    Parameters
    ----------
    missing_values:
        True -> keep value in oldcol
        'str' -> replace by string
        np.nan -> replace by np.nan

    """
    if missing_values is True:
        df[newcol] = df[oldcol].map(column_by_dict).fillna(df[oldcol])
    elif missing_values is np.nan:
        df[newcol] = df[oldcol].map(column_by_dict)
    elif type(missing_values) is str:
        df[newcol] = df[oldcol].map(column_by_dict).fillna(missing_values)
    else:
        raise KeyError('wrong datatype for missing_values')


# %% convenience routine (to combine all functions)

def import_file(function='', perform_stack_to_hgc=True, **kwargs):
    """
    Convenience function to import file and convert to HGC format.

    Parameters
    ----------
    function: string
        IF 'read_stacked': use read_stack() function to read data
        IF 'read_wide': use read_wide() function to read data
        IF 'read_gef': use read_gef() function to read data
    perform_stack_to_hgc: bool
        IF True: generate wide dataframe used by HGC
        IF False: do not generate wide dataframe (save computation)
        default: True

    Other parameter
    ---------------
    See functions:
        read_stack(), read_wide() or read_gef()
        hgc.ner.generate_feature_map()
        hgc.ner.generate_feature_map()
        harmonize()
        stack_to_hgc()

    Return
    ------
        df_harmonized:
            stacked dataframe, return of hgc.io.harmonize()
        df_hgc:
            wide dataframe, return of stack_to_hgc()
        arguments: dictionary
            feature_mapped, feature_unmapped, df_feature_map
            feature_mapped, feature_unmapped, df_feature_map
            df_dropna, df_dropduplicates,
            Parameters

    """
    # generate a dictionary with all arguments passed in the function
    arguments = copy.deepcopy({**locals(), **locals()['kwargs']})

    # read data to stacked dataframe
    if function == 'read_stacked':
        arguments['df_read'] = read_stacked(**arguments)
    elif function == 'read_wide':
        arguments['df_read'] = read_wide(**arguments)
    elif function == 'read_gef':
        arguments['df_read'] = read_gef(**arguments)
    else:
        print('ERROR: unknown functions')

    # # map features
    # arguments['entity_orig'] = list(set(arguments['df_read']['Feature']))
    # arguments['feature_map'], arguments['feature_unmapped'], arguments['df_feature_map'] =\ ####################################
    #     hgc.ner.generate_feature_map(**arguments)

    # # map units
    # arguments['unit_orig'] = list(set(arguments['df_read']['Unit']))
    # arguments['unit_map'], arguments['unit_unmapped'], arguments['df_unit_map'] =\ ########################################
    #     hgc.ner.generate_unit_map(**arguments)

    # harmonize
    arguments['df_harmonized'], dct = harmonize(**arguments)
    arguments = {**arguments, **dct}

    # stack_to_hgc
    if arguments['perform_stack_to_hgc'] == True:
        arguments['df_hgc'] = stack_to_hgc(**arguments)
    else:
        arguments['df_hgc'] = pd.DataFrame([])  # return empty df

    return arguments['df_harmonized'], arguments['df_hgc'], arguments